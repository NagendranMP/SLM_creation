{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0d10790-f08a-4995-9761-d935d86292b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a896914-09a1-470e-9e20-a63059d69f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tinyLlama model ,this is one of the hugging face model and small language model as well\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a3dcf04-c998-4570-b3f9-e7f19fd30270",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b11095818e44658854266bfb716186f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name) # tokenizing the model means convert the text into the meaning full number.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ") # before this process the output has been stored in the list format but now it is changing into tensor format that means array format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02ce48e0-221b-4d9e-93a4-1fa7a8d7bedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(question):  # creating function for Q&A process\n",
    "    prompt = f\"\"\"\n",
    "You are a helpful AI assistant.\n",
    "Answer the following question clearly and briefly.\n",
    "\n",
    "Question: {question} \n",
    "Answer:\n",
    "\"\"\" # our questions will be stord here\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\") # our qustions will be tokenizing here \n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=150,\n",
    "        temperature=0.7\n",
    "    ) #  tokenized inputs are gone into the model \n",
    "\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True) #now decoding process happening here that means number convert into text\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff714a89-b43b-4160-9302-5ef55bde9a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a helpful AI assistant.\n",
      "Answer the following question clearly and briefly.\n",
      "\n",
      "Question: What is Small Language model? \n",
      "Answer:\n",
      "Small Language model is a type of AI model that can learn and understand natural language. It is a type of machine learning model that can be trained on large amounts of text data to perform tasks such as natural language processing (NLP) and speech recognition. Small Language models are often used in applications such as chatbots, language translation, and speech recognition.\n"
     ]
    }
   ],
   "source": [
    "response = ask_question(\"What is Small Language model?\") # now calling the function\n",
    "print(response) # print the  output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74f3026-4fdb-467d-8b29-02568a0d68b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d820e8a-6b27-406d-b374-e187a96e2b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sales: 5600631.960000001\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel(\"50_Startups.xlsx\")\n",
    "print(\"Total Sales:\", df[\"Profit\"].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0e5d6e2-ebfe-4bb6-b5e9-3985c62eb1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a data assistant.\n",
      "\n",
      "Available columns:\n",
      "['R&D Spend', 'Administration', 'Marketing Spend', 'State', 'Profit']\n",
      "\n",
      "From the question below, return ONLY the exact column name that matches.\n",
      "Return only one column name.\n",
      "Do not explain.\n",
      "Do not generate code.\n",
      "\n",
      "Question: What is the sum of the Profit?\n",
      "Answer:\n",
      "Profit = R&D Spend + Administration + Marketing Spend\n",
      "Profit = 1000 + 500 + 1000\n",
      "Profit = 5000\n",
      "Profit = 500\n",
      "Profit = 500\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the sum of the Profit?\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "You are a data assistant.\n",
    "\n",
    "Available columns:\n",
    "{list(df.columns)}\n",
    "\n",
    "From the question below, return ONLY the exact column name that matches.\n",
    "Return only one column name.\n",
    "Do not explain.\n",
    "Do not generate code.\n",
    "\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=60, temperature=0.2)\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b043562-99c3-46fb-8b2e-7a8c49222b53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c3e7049252b4fb2b40a04905f6205a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/661 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python313\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ADMIN\\.cache\\huggingface\\hub\\models--Qwen--Qwen2.5-3B-Instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b9dafbce2e241c4a37c702144bf0f0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adc688df30f946b985d027d745f23dc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed282822924e49c9ab462a2f62704277",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b64f900b7b0f414d99e3b9b2a36f75a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8368360fff148408300476fee65691b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f583ac40f8e347f4adfaa692443e60f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "745500afdab2431b9cd8cf6f7399ac47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5be5dec9911447f7a67ea41d69d5ea3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/434 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "381890548bda4afeb875e73e7f223751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4810fe4-5ad9-47c2-8d1b-f95dbb8efb0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sales: 5600631.960000001\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel(\"50_Startups.xlsx\")\n",
    "print(\"Total Sales:\", df[\"Profit\"].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d8b3600-e68c-4010-a508-9874d5c44a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a pandas DataFrame expert.\n",
      "\n",
      "DataFrame name: df\n",
      "Available columns:\n",
      "['R&D Spend', 'Administration', 'Marketing Spend', 'State', 'Profit']\n",
      "\n",
      "Your task:\n",
      "Understand the question semantically and generate\n",
      "a single valid pandas expression that returns the final answer.\n",
      "\n",
      "Guidelines:\n",
      "- If the question asks for the number of distinct categories or unique values, use nunique().\n",
      "- If the question asks for a total or overall amount, use sum().\n",
      "- If the question refers to a specific person or category, filter the dataframe first before selecting the column.\n",
      "- If aggregation is required, apply it after filtering.\n",
      "- Always return exactly one valid pandas expression.\n",
      "- Do not explain.\n",
      "- Do not assign variables.\n",
      "- Do not modify df.\n",
      "- Do not print anything.\n",
      "\n",
      "Examples:\n",
      "df[\"State\"].nunique()\n",
      "df[\"Profit\"].sum()\n",
      "df[df[\"Name\"] == \"Ravi\"][\"Profit\"].values[0]\n",
      "\n",
      "Question: How many times the Florida states appear?\n",
      "Answer:\n",
      "df[df[\"State\"] == \"Florida\"][\"State\"].nunique()\n"
     ]
    }
   ],
   "source": [
    "question = \"How many times the Florida states appear?\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "You are a pandas DataFrame expert.\n",
    "\n",
    "DataFrame name: df\n",
    "Available columns:\n",
    "{list(df.columns)}\n",
    "\n",
    "Your task:\n",
    "Understand the question semantically and generate\n",
    "a single valid pandas expression that returns the final answer.\n",
    "\n",
    "Guidelines:\n",
    "- If the question asks for the number of distinct categories or unique values, use nunique().\n",
    "- If the question asks for a total or overall amount, use sum().\n",
    "- If the question refers to a specific person or category, filter the dataframe first before selecting the column.\n",
    "- If aggregation is required, apply it after filtering.\n",
    "- Always return exactly one valid pandas expression.\n",
    "- Do not explain.\n",
    "- Do not assign variables.\n",
    "- Do not modify df.\n",
    "- Do not print anything.\n",
    "\n",
    "Examples:\n",
    "df[\"State\"].nunique()\n",
    "df[\"Profit\"].sum()\n",
    "df[df[\"Name\"] == \"Ravi\"][\"Profit\"].values[0]\n",
    "\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=60, temperature=0.2)\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70a5fb1d-4798-4aa3-bd10-e0803c0c32c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"State\"] == \"Florida\"][\"State\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a174aab-f145-42b2-b66c-7a220beeab44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
